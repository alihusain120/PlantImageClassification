{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9ZmxP6StAoz"
   },
   "source": [
    "# CS542 - Class Challenge - fine-grained classification of plants:\n",
    "\n",
    "Our class challenge will consists of two tasks addressing an image recognition task where our dataset contains about 1K categories of plants with only about 250,000 images.  There will be two parts to this task:\n",
    "\n",
    "1. Image classification. Imagine we have cateloged all the plants we care to identify, now we just need to create a classifier for them! Use your skills from the supervised learning sections of this course to try to address this problem.\n",
    "\n",
    "2. Semi-Supervised/Few-Shot Learning.  Unfortunately, we missed some important plants we want to classify!  We do have some images we think contain the plant, but we have only have a few labels.  Our new goal is to develop an AI model that can learn from just these labeled examples.\n",
    "\n",
    "Each student must submit a model on both tasks.  Students in the top 3 on each task will get 5% extra credit on this assignment.\n",
    "\n",
    "This notebook is associated with the second task (semi-supervised).\n",
    "\n",
    "\n",
    "# Dataset\n",
    "The dataset is downloaded on scc in the address: \"/projectnb2/cs542-bap/classChallenge/data\". You can find the python version of this notebook there as well or you could just type \"jupyter nbconvert --to script baselineModel_task2.ipynb\" and it will output \"baselineModel_task2.py\". You should be able to run \"baselineModel_task2.py\" on scc by simply typing \"python baselineModel_task2.py\"\n",
    "\n",
    "Please don't try to change or delete the dataset.\n",
    "\n",
    "# Evaluation:\n",
    "You will compete with each other over your performance on the dedicated test set. The performance measure is classification accuracy, i.e: if the true class is your top predictions. \n",
    "\n",
    "# Baseline:\n",
    "The following code is a baseline which you can use and improve to come up with your model for this task\n",
    "\n",
    "# Suggestion\n",
    "One simple suggestion would be to use a pretrained model on imagenet and finetune it on this data similar to this [link](https://keras.io/api/applications/)\n",
    "Also you should likely train more than 2 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q8oub7ntAo1"
   },
   "source": [
    "## Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "14D2EZ17tAo1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "Num GPUs Available:  1\n",
      "3\n",
      "NUM CORES:  4\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(os.getenv(\"CUDA_VISIBLE_DEVICES\"))\n",
    "tf.config.set_soft_device_placement(True)\n",
    "def get_n_cores():\n",
    "  nslots = os.getenv('NSLOTS')\n",
    "  if nslots is not None:\n",
    "    return int(nslots)\n",
    "  raise ValueError('Environment variable NSLOTS is not defined.')\n",
    "print(\"NUM CORES: \", get_n_cores())\n",
    "tf.config.threading.set_intra_op_parallelism_threads(get_n_cores()-1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYaBUsR-tAo3"
   },
   "source": [
    "# Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "m893cNgztAo3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/projectnb2/cs542-bap/class_challenge/'\n",
    "\n",
    "train_ds = tf.data.TextLineDataset(os.path.join(data_dir, 'train_held_out_labeled.txt'))\n",
    "train_unlabeled_ds = tf.data.TextLineDataset(os.path.join(data_dir, 'train_held_out.txt'))\n",
    "val_ds = tf.data.TextLineDataset(os.path.join(data_dir, 'val_held_out.txt'))\n",
    "test_ds = tf.data.TextLineDataset(os.path.join(data_dir, 'test_held_out.txt'))\n",
    "\n",
    "with open(os.path.join(data_dir, 'classes_held_out.txt'), 'r') as f:\n",
    "  class_names = [c.strip() for c in f.readlines()]\n",
    "\n",
    "num_classes = len(class_names)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2m6getXwtAo3"
   },
   "source": [
    "## Write a short function that converts a file path to an (img, label) pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZrIrN5iItAo3"
   },
   "outputs": [],
   "source": [
    "def decode_img(img, crop_size=224):\n",
    "  img = tf.io.read_file(img)\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # resize the image to the desired size\n",
    "  return tf.image.resize(img, [crop_size, crop_size])\n",
    "\n",
    "def get_label(label):\n",
    "  # find teh matching label\n",
    "  one_hot = tf.where(tf.equal(label, class_names))\n",
    "  # Integer encode the label\n",
    "  return tf.reduce_min(one_hot)\n",
    "\n",
    "def process_path(file_path):\n",
    "  # should have two parts\n",
    "  file_path = tf.strings.split(file_path)\n",
    "  # second part has the class index\n",
    "  label = get_label(file_path[1])\n",
    "  # load the raw data from the file\n",
    "  img = decode_img(tf.strings.join([data_dir, 'images/', file_path[0], '.jpg']))\n",
    "  return img, label\n",
    "\n",
    "def process_path_unlabeled(file_path):\n",
    "  # load the raw data from the file\n",
    "  img = decode_img(tf.strings.join([data_dir, 'images/', file_path, '.jpg']))\n",
    "  #WE are using this method for unlabeled data, so we will manually return -1 as a label for scikit clustering\n",
    "  return img, tf.cast(-1., tf.int64)\n",
    "\n",
    "def process_path_test(file_path):\n",
    "  # load the raw data from the file\n",
    "  img = decode_img(tf.strings.join([data_dir, 'images/', file_path, '.jpg']))\n",
    "  return img, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBUt-k32tAo3"
   },
   "source": [
    "# Finish setting up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hD7gOwX0tAo3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function process_path at 0x2b4c97ec9440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function process_path at 0x2b4c97ec9440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "\n",
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "#train_unlabeled_ds = train_unlabeled_ds.map(process_path_unlabeled, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.map(process_path_test, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "#Concatenate labeled and unlabeled for one training dataset\n",
    "####train_ds = train_ds.concatenate(train_unlabeled_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN to label unlabled data points, skip"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Split train_ds to X, y array-like for Sci-Kit learn KNN\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for img, lbl in train_ds:\n",
    "    X.append(img.numpy().flatten())\n",
    "    y.append(lbl)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.semi_supervised import LabelPropagation\n",
    "\n",
    "n_total_samples = 4308\n",
    "n_labeled_points = 100\n",
    "\n",
    "lp_model = LabelPropagation()\n",
    "lp_model.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Unfinished Mark\n",
    "\n",
    "#TRAIN_DS (Labeled) DATA POINTS = 100\n",
    "#TRAIN__UNLABELED_DS DATA POINTS = 4208\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "lp_model = LabelPropagation(kernel='knn', n_neighbors=20, max_iter=20)\n",
    "lp_model.fit(X, y)\n",
    "\n",
    "print(y[4208:]) #test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npredicted_labels = lp_model.transduction_[unlabeled_set]\\ntrue_labels = y[unlabeled_set]\\n\\ncm = confusion_matrix(true_labels, predicted_labels, labels=lp_model.classes_)\\n\\nprint(\"Label Spreading model: %d labeled & %d unlabeled points (%d total)\" %\\n      (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))\\n\\nprint(classification_report(true_labels, predicted_labels))\\n\\nprint(\"Confusion matrix\")\\nprint(cm)\\n\\n# #############################################################################\\n# Calculate uncertainty values for each transduced distribution\\npred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)\\n\\n# #############################################################################\\n# Pick the top 10 most uncertain labels\\nuncertainty_index = np.argsort(pred_entropies)[-10:]\\n\\n# #############################################################################\\n# Plot\\nf = plt.figure(figsize=(7, 5))\\nfor index, image_index in enumerate(uncertainty_index):\\n    image = images[image_index]\\n\\n    sub = f.add_subplot(2, 5, index + 1)\\n    sub.imshow(image, cmap=plt.cm.gray_r)\\n    plt.xticks([])\\n    plt.yticks([])\\n    sub.set_title(\\'predict: %i\\ntrue: %i\\' % (\\n        lp_model.transduction_[image_index], y[image_index]))\\n\\nf.suptitle(\\'Learning with small amount of labeled data\\')\\nplt.show()\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adapted from scikit learn docs\n",
    "'''\n",
    "predicted_labels = lp_model.transduction_[unlabeled_set]\n",
    "true_labels = y[unlabeled_set]\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels, labels=lp_model.classes_)\n",
    "\n",
    "print(\"Label Spreading model: %d labeled & %d unlabeled points (%d total)\" %\n",
    "      (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "print(\"Confusion matrix\")\n",
    "print(cm)\n",
    "\n",
    "# #############################################################################\n",
    "# Calculate uncertainty values for each transduced distribution\n",
    "pred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)\n",
    "\n",
    "# #############################################################################\n",
    "# Pick the top 10 most uncertain labels\n",
    "uncertainty_index = np.argsort(pred_entropies)[-10:]\n",
    "\n",
    "# #############################################################################\n",
    "# Plot\n",
    "f = plt.figure(figsize=(7, 5))\n",
    "for index, image_index in enumerate(uncertainty_index):\n",
    "    image = images[image_index]\n",
    "\n",
    "    sub = f.add_subplot(2, 5, index + 1)\n",
    "    sub.imshow(image, cmap=plt.cm.gray_r)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    sub.set_title('predict: %i\\ntrue: %i' % (\n",
    "        lp_model.transduction_[image_index], y[image_index]))\n",
    "\n",
    "f.suptitle('Learning with small amount of labeled data')\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentFlipUD(img, label):\n",
    "    return tf.image.flip_up_down(img), label\n",
    "\n",
    "def augmentFlipLR(img, label):\n",
    "    return tf.image.flip_left_right(img), label\n",
    "\n",
    "def augmentRotate(img, label):\n",
    "    if np.random.random() < .5:\n",
    "        return tf.image.rot90(img, k=1), label\n",
    "    else:\n",
    "        return tf.image.rot90(img, k=3), label\n",
    "    \n",
    "#Let's augment and grow our training set 4x\n",
    "\n",
    "tdsAugment = train_ds.map(augmentFlipUD, num_parallel_calls=AUTOTUNE)\n",
    "tdsAugment = tdsAugment.concatenate(train_ds.map(augmentFlipLR, num_parallel_calls=AUTOTUNE))\n",
    "tdsAugment = tdsAugment.concatenate(train_ds.map(augmentRotate, num_parallel_calls=AUTOTUNE))\n",
    "\n",
    "train_ds = train_ds.concatenate(tdsAugment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epSS5nlYtAo3"
   },
   "source": [
    "## Data loader hyper-parameters for performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oIrk9SuYtAo3"
   },
   "outputs": [],
   "source": [
    "def configure_for_performance(ds):\n",
    "  ds = ds.cache()\n",
    "  ds = ds.shuffle(buffer_size=1000)\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "  return ds\n",
    "\n",
    "train_ds = configure_for_performance(train_ds)\n",
    "#train_unlabeled_ds = configure_for_performance(train_unlabeled_ds)\n",
    "val_ds = configure_for_performance(val_ds)\n",
    "test_ds = configure_for_performance(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOA90IRtAo4"
   },
   "source": [
    "## Transfer Learning from Task 1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wWHpzQoatAo4"
   },
   "outputs": [],
   "source": [
    "#Load and use model from task 1\n",
    "task1_model = keras.models.load_model('baselineAugmented.hdf5')\n",
    "\n",
    "#Set base to untrainable\n",
    "task1_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "x = task1_model(inputs, training=False)\n",
    "outputs = keras.layers.Dense(20)(x)\n",
    "model = keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CFFrdvMtAo4"
   },
   "source": [
    "## The usual loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MaUMyxl3tAo4"
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(\n",
    "  optimizer=opt,\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy',tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYePw6F3tAo4"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "BEi56DBrtAo4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "     16/Unknown - 0s 10ms/step - loss: 5.3874 - accuracy: 0.0925 - sparse_top_k_categorical_accuracy: 0.2600\n",
      "Epoch 00001: loss improved from inf to 5.38738, saving model to task2.hdf5\n",
      "16/16 [==============================] - 13s 799ms/step - loss: 5.3874 - accuracy: 0.0925 - sparse_top_k_categorical_accuracy: 0.2600 - val_loss: 4.2679 - val_accuracy: 0.0776 - val_sparse_top_k_categorical_accuracy: 0.4323\n",
      "Epoch 2/40\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 4.0777 - accuracy: 0.1385 - sparse_top_k_categorical_accuracy: 0.4123\n",
      "Epoch 00002: loss improved from 5.38738 to 4.02875, saving model to task2.hdf5\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 4.0287 - accuracy: 0.1375 - sparse_top_k_categorical_accuracy: 0.4150 - val_loss: 3.4428 - val_accuracy: 0.1551 - val_sparse_top_k_categorical_accuracy: 0.5429\n",
      "Epoch 3/40\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 3.2441 - accuracy: 0.2062 - sparse_top_k_categorical_accuracy: 0.5569\n",
      "Epoch 00003: loss improved from 4.02875 to 3.12673, saving model to task2.hdf5\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 3.1267 - accuracy: 0.2150 - sparse_top_k_categorical_accuracy: 0.5800 - val_loss: 2.9106 - val_accuracy: 0.2310 - val_sparse_top_k_categorical_accuracy: 0.6419\n",
      "Epoch 4/40\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 2.4940 - accuracy: 0.3169 - sparse_top_k_categorical_accuracy: 0.7169\n",
      "Epoch 00004: loss improved from 3.12673 to 2.47555, saving model to task2.hdf5\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 2.4755 - accuracy: 0.3225 - sparse_top_k_categorical_accuracy: 0.7050 - val_loss: 2.5438 - val_accuracy: 0.2987 - val_sparse_top_k_categorical_accuracy: 0.7063\n",
      "Epoch 5/40\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.9933 - accuracy: 0.4431 - sparse_top_k_categorical_accuracy: 0.7908\n",
      "Epoch 00005: loss improved from 2.47555 to 2.00714, saving model to task2.hdf5\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 2.0071 - accuracy: 0.4500 - sparse_top_k_categorical_accuracy: 0.7825 - val_loss: 2.2739 - val_accuracy: 0.3416 - val_sparse_top_k_categorical_accuracy: 0.7739\n",
      "Epoch 6/40\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.7863 - accuracy: 0.4923 - sparse_top_k_categorical_accuracy: 0.8154\n",
      "Epoch 00006: loss improved from 2.00714 to 1.69906, saving model to task2.hdf5\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 1.6991 - accuracy: 0.5300 - sparse_top_k_categorical_accuracy: 0.8300 - val_loss: 2.1048 - val_accuracy: 0.3762 - val_sparse_top_k_categorical_accuracy: 0.8020\n",
      "Epoch 7/40\n",
      "13/16 [=======================>......] - ETA: 0s - loss: 1.4580 - accuracy: 0.5785 - sparse_top_k_categorical_accuracy: 0.8677\n",
      "Epoch 00007: loss improved from 1.69906 to 1.47160, saving model to task2.hdf5\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 1.4716 - accuracy: 0.5850 - sparse_top_k_categorical_accuracy: 0.8625 - val_loss: 1.9743 - val_accuracy: 0.3993 - val_sparse_top_k_categorical_accuracy: 0.8317\n",
      "Epoch 8/40\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.3036 - accuracy: 0.6250 - sparse_top_k_categorical_accuracy: 0.8850\n",
      "Epoch 00008: loss improved from 1.47160 to 1.30362, saving model to task2.hdf5\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 1.3036 - accuracy: 0.6250 - sparse_top_k_categorical_accuracy: 0.8850 - val_loss: 1.8652 - val_accuracy: 0.4389 - val_sparse_top_k_categorical_accuracy: 0.8482\n",
      "Epoch 9/40\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.1692 - accuracy: 0.6425 - sparse_top_k_categorical_accuracy: 0.9000\n",
      "Epoch 00009: loss improved from 1.30362 to 1.16916, saving model to task2.hdf5\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 1.1692 - accuracy: 0.6425 - sparse_top_k_categorical_accuracy: 0.9000 - val_loss: 1.7903 - val_accuracy: 0.4571 - val_sparse_top_k_categorical_accuracy: 0.8531\n",
      "Epoch 10/40\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0607 - accuracy: 0.6825 - sparse_top_k_categorical_accuracy: 0.9050\n",
      "Epoch 00010: loss improved from 1.16916 to 1.06072, saving model to task2.hdf5\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 1.0607 - accuracy: 0.6825 - sparse_top_k_categorical_accuracy: 0.9050 - val_loss: 1.7252 - val_accuracy: 0.4686 - val_sparse_top_k_categorical_accuracy: 0.8564\n",
      "Epoch 11/40\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.9725 - accuracy: 0.7200 - sparse_top_k_categorical_accuracy: 0.9125\n",
      "Epoch 00011: loss improved from 1.06072 to 0.97251, saving model to task2.hdf5\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 0.9725 - accuracy: 0.7200 - sparse_top_k_categorical_accuracy: 0.9125 - val_loss: 1.6958 - val_accuracy: 0.4901 - val_sparse_top_k_categorical_accuracy: 0.8581\n",
      "Epoch 12/40\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8945 - accuracy: 0.7525 - sparse_top_k_categorical_accuracy: 0.9225\n",
      "Epoch 00012: loss improved from 0.97251 to 0.89448, saving model to task2.hdf5\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.8945 - accuracy: 0.7525 - sparse_top_k_categorical_accuracy: 0.9225 - val_loss: 1.6592 - val_accuracy: 0.5132 - val_sparse_top_k_categorical_accuracy: 0.8581\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"task2.hdf5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta = .1, patience=2)\n",
    "\n",
    "first_iter_history = model.fit(train_ds,validation_data=val_ds,epochs=40,shuffle=True, callbacks=[checkpoint, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'sparse_top_k_categorical_accuracy', 'val_loss', 'val_accuracy', 'val_sparse_top_k_categorical_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(first_iter_history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(first_iter_history.history['loss'])\n",
    "plt.plot(first_iter_history.history['val_loss'])\n",
    "plt.plot(first_iter_history.history['sparse_top_k_categorical_accuracy'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val', 'k-acc'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "task2_model = keras.models.load_model('task2.hdf5')\n",
    "\n",
    "#Pseudo Labeling\n",
    "\n",
    "dict_labels = {}\n",
    "for image_batch, image_names in train_unlabeled_ds:\n",
    "    predictions = task2_model.predict(image_batch)\n",
    "    for image_name, predictions in zip(image_names.numpy(), task2_model.predict(image_batch)):\n",
    "      inds = np.argmax(predictions)\n",
    "      dict_labels[str(int(image_name))] = class_names[inds]\n",
    "      "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for x in list(dict_labels)[:3]:\n",
    "    print(type(x))\n",
    "    print(type(dict_labels[x]))\n",
    "    \n",
    "print(train_unlabeled_ds.take(1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def getPseudoLabels(img, label):\n",
    "    return img, dict_labels[label.numpy()]\n",
    "\n",
    "train_unl = train_unl.map(getPseudoLabels, num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.concatenate(train_unl)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "checkpoint = ModelCheckpoint(\"task2_2.hdf5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta = .1, patience=2)\n",
    "\n",
    "second_iter_history = model.fit(train_ds,validation_data=val_ds,epochs=10,shuffle=True, callbacks=[checkpoint, es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpPs4SWetAo4"
   },
   "source": [
    "# Output submission csv for Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sV2YebpItAo4"
   },
   "outputs": [],
   "source": [
    "#new_model = keras.models.load_model('baselineAugmented.hdf5')\n",
    "#with open('submission_task2_2_semisupervised.csv', 'w') as f:\n",
    "#new_model = keras.models.load_model('task2.hdf5')\n",
    "\n",
    "    \n",
    "with open('submission_task2_semisupervised.csv', 'w') as f:\n",
    "  f.write('id,predicted\\n')\n",
    "  for image_batch, image_names in test_ds:\n",
    "    predictions = model.predict(image_batch)\n",
    "    for image_name, predictions in zip(image_names.numpy(), model.predict(image_batch)):\n",
    "      inds = np.argmax(predictions)\n",
    "      line = str(int(image_name)) + ',' + class_names[inds]\n",
    "      f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xt7OaOehvoh1"
   },
   "source": [
    "**Note**\n",
    "\n",
    "Absolute path is recommended here. For example, use \"/projectnb2/cs542-bap/[your directory name]/submission_task2_supervised.csv\" to replace \"submission_task2_supervised.csv\".\n",
    "\n",
    "Besides, you can request good resources by specify the type of gpus, such as \"qsub -l gpus=1 -l gpu_type=P100 [your file name].qsub\". This is helpful to avoid potential issues of GPUs, such as out of memory, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "baselineModel.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
